---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.4.2
  kernelspec:
    display_name: miRNAselector
    language: R
    name: ir
---

```{r echo = FALSE, message = FALSE, warning = FALSE}
library(miRNAselector)
dane = ks.load_datamix(use_smote_not_rose = T)  # load mixed_*.csv files
train = dane[[1]]
test = dane[[2]]
valid = dane[[3]]
train_smoted = dane[[4]]
trainx = dane[[5]]
trainx_smoted = dane[[6]]  # get the objects from list to make the code more readable.

mixed = data.table::fread("mixed.csv")
de = ks.miRNA_differential_expression(trainx, train$Class)
```

### Check overall methods performance:

```{r echo = FALSE, message = FALSE, warning = FALSE}
metody = ks.get_benchmark_methods("benchmark.csv")  # gets the methods used in benchmark
par(mfrow = c(2, 2))
for (i in 1:length(metody)) {
    temp = ks.get_benchmark("benchmark.csv")  # loads benchmark
    temp2 = dplyr::select(temp, starts_with(paste0(metody[i], "_")))
    boxplot(temp[, paste0(metody[i], "_train_Accuracy")], temp[, paste0(metody[i], "_test_Accuracy")], temp[, paste0(metody[i], "_valid_Accuracy")], main = paste0("Method: ",
        metody[i]), names = c("Training", "Testing", "Validation"), ylab = "Accuracy", ylim = c(0.5, 1))
    tempids = c(match(paste0(metody[i], "_train_Accuracy"), colnames(temp)), match(paste0(metody[i], "_test_Accuracy"), colnames(temp)), match(paste0(metody[i],
        "_valid_Accuracy"), colnames(temp)))
}
par(mfrow = c(1, 1))
```

## Best set proposals

### Top 3 sets which achived the best accuracy in training, testing and validation:

(metaindex = mean of all 3 accuracy metrics)

```{r echo = FALSE, message = FALSE, warning = FALSE}
acc = ks.best_signiture_proposals(benchmark_csv = "benchmark.csv", without_train = F)  # generates the benchmark sorted by metaindex
best_signatures = acc[1:3, ]  # get top 3 methods
ks.table(best_signatures[, c("metaindex", "method", "miRy")])
```

Performance of those signitures:

```{r echo = FALSE, message = FALSE, warning = FALSE}
for (i in 1:3) {
    # cat(paste0("\n\n## ", acc$method[i], "\n\n"))
    par(mfrow = c(1, 2))
    # acc = ks.best_signiture_proposals_meta11("benchmark.csv")
    metody = ks.get_benchmark_methods("benchmark.csv")
    ktory_set = match(acc$method[i], ks.get_benchmark("benchmark.csv")$method)
    # do_ktorej_kolumny = which(colnames(acc) == 'metaindex') barplot(as.numeric(acc[i,1:do_ktorej_kolumny]))
    for (ii in 1:length(metody)) {

        temp = ks.get_benchmark("benchmark.csv") %>% dplyr::select(starts_with(paste0(metody[ii], "_t")), starts_with(paste0(metody[ii], "_v")))

        ROCtext = paste0("Training AUC ROC: ", round(temp[ktory_set, 1], 2), " (95%CI: ", round(temp[ktory_set, 2], 2), "-", round(temp[ktory_set, 3],
            2), ")")

        temp = temp[, -c(1:3)]
        temp2 = as.numeric(temp[ktory_set, ])
        temp3 = matrix(temp2, nrow = 3, byrow = T)
        colnames(temp3) = c("Accuracy", "Sensitivity", "Specificity")
        rownames(temp3) = c("Training", "Testing", "Validation")
        temp3 = t(temp3)

        plot1 = barplot(temp3, beside = T, ylim = c(0, 1), xlab = paste0(ROCtext, "\nBlack - accuracy, blue - sensitivity, green - specificity"), width = 0.85,
            col = c("black", "blue", "green"), legend = F, args.legend = list(x = "topright", bty = "n", inset = c(0, -0.25)), cex.lab = 0.7, main = paste0(acc$method[i],
                " - ", metody[ii]), font.lab = 2)
        ## Add text at top of bars
        text(x = plot1, y = as.numeric(temp3), label = paste0(round(as.numeric(temp[ktory_set, ]) * 100, 1), "%"), pos = 3, cex = 0.6, col = "red")
    }
    par(mfrow = c(1, 1))

}
```

Overlap of those signitures:

```{r echo = FALSE, message = FALSE, warning = FALSE}
overlap = ks.miRNA_signiture_overlap(acc$method[1:3], "benchmark.csv")
attr(overlap, "intersections")
```

### Top 3 sets which achived the best accuracy in testing and validation: 

(metaindex = mean of 2 accuracy metrics)

```{r echo = FALSE, message = FALSE, warning = FALSE}
acc = ks.best_signiture_proposals(benchmark_csv = "benchmark.csv", without_train = T)  # generates the benchmark sorted by metaindex
best_signatures = acc[1:3, ]  # get top 3 methods
ks.table(best_signatures[, c("metaindex", "method", "miRy")])
```

Performance of those signitures:

```{r echo = FALSE, message = FALSE, warning = FALSE}
for (i in 1:3) {
    # cat(paste0("\n\n## ", acc$method[i], "\n\n"))
    par(mfrow = c(1, 2))
    # acc = ks.best_signiture_proposals_meta11("benchmark.csv")
    metody = ks.get_benchmark_methods("benchmark.csv")
    ktory_set = match(acc$method[i], ks.get_benchmark("benchmark.csv")$method)
    # do_ktorej_kolumny = which(colnames(acc) == 'metaindex') barplot(as.numeric(acc[i,1:do_ktorej_kolumny]))
    for (ii in 1:length(metody)) {

        temp = ks.get_benchmark("benchmark.csv") %>% dplyr::select(starts_with(paste0(metody[ii], "_t")), starts_with(paste0(metody[ii], "_v")))

        ROCtext = paste0("Training AUC ROC: ", round(temp[ktory_set, 1], 2), " (95%CI: ", round(temp[ktory_set, 2], 2), "-", round(temp[ktory_set, 3],
            2), ")")

        temp = temp[, -c(1:3)]
        temp2 = as.numeric(temp[ktory_set, ])
        temp3 = matrix(temp2, nrow = 3, byrow = T)
        colnames(temp3) = c("Accuracy", "Sensitivity", "Specificity")
        rownames(temp3) = c("Training", "Testing", "Validation")
        temp3 = t(temp3)

        plot1 = barplot(temp3, beside = T, ylim = c(0, 1), xlab = paste0(ROCtext, "\nBlack - accuracy, blue - sensitivity, green - specificity"), width = 0.85,
            col = c("black", "blue", "green"), legend = F, args.legend = list(x = "topright", bty = "n", inset = c(0, -0.25)), cex.lab = 0.7, main = paste0(acc$method[i],
                " - ", metody[ii]), font.lab = 2)
        ## Add text at top of bars
        text(x = plot1, y = as.numeric(temp3), label = paste0(round(as.numeric(temp[ktory_set, ]) * 100, 1), "%"), pos = 3, cex = 0.6, col = "red")
    }
    par(mfrow = c(1, 1))

}
```

Overlap of those signitures:

```{r echo = FALSE, message = FALSE, warning = FALSE}
overlap = ks.miRNA_signiture_overlap(acc$method[1:3], "benchmark.csv")
attr(overlap, "intersections")
```

### Top 3 sets which achived the best sensitivity and specificity in validation: 

(metaindex = mean of sensivitiy and specificity in validation dataset)

```{r echo = FALSE, message = FALSE, warning = FALSE}
acc = ks.best_signiture_proposals_meta11(benchmark_csv = "benchmark.csv")  # generates the benchmark sorted by metaindex
best_signatures = acc[1:3, ]  # get top 3 methods
ks.table(best_signatures[, c("metaindex", "method", "miRy")])
```

Performance of those signitures:

```{r echo = FALSE, message = FALSE, warning = FALSE}
for (i in 1:3) {
    # cat(paste0("\n\n## ", acc$method[i], "\n\n"))
    par(mfrow = c(1, 2))
    # acc = ks.best_signiture_proposals_meta11("benchmark.csv")
    metody = ks.get_benchmark_methods("benchmark.csv")
    ktory_set = match(acc$method[i], ks.get_benchmark("benchmark.csv")$method)
    # do_ktorej_kolumny = which(colnames(acc) == 'metaindex') barplot(as.numeric(acc[i,1:do_ktorej_kolumny]))
    for (ii in 1:length(metody)) {

        temp = ks.get_benchmark("benchmark.csv") %>% dplyr::select(starts_with(paste0(metody[ii], "_t")), starts_with(paste0(metody[ii], "_v")))

        ROCtext = paste0("Training AUC ROC: ", round(temp[ktory_set, 1], 2), " (95%CI: ", round(temp[ktory_set, 2], 2), "-", round(temp[ktory_set, 3],
            2), ")")

        temp = temp[, -c(1:3)]
        temp2 = as.numeric(temp[ktory_set, ])
        temp3 = matrix(temp2, nrow = 3, byrow = T)
        colnames(temp3) = c("Accuracy", "Sensitivity", "Specificity")
        rownames(temp3) = c("Training", "Testing", "Validation")
        temp3 = t(temp3)

        plot1 = barplot(temp3, beside = T, ylim = c(0, 1), xlab = paste0(ROCtext, "\nBlack - accuracy, blue - sensitivity, green - specificity"), width = 0.85,
            col = c("black", "blue", "green"), legend = F, args.legend = list(x = "topright", bty = "n", inset = c(0, -0.25)), cex.lab = 0.7, main = paste0(acc$method[i],
                " - ", metody[ii]), font.lab = 2)
        ## Add text at top of bars
        text(x = plot1, y = as.numeric(temp3), label = paste0(round(as.numeric(temp[ktory_set, ]) * 100, 1), "%"), pos = 3, cex = 0.6, col = "red")
    }
    par(mfrow = c(1, 1))

}
```

Overlap of those signitures:

```{r echo = FALSE, message = FALSE, warning = FALSE}
overlap = ks.miRNA_signiture_overlap(acc$method[1:3], "benchmark.csv")
attr(overlap, "intersections")
```

## General analysis

### Overfitting analysis:

```{r echo = FALSE, message = FALSE, warning = FALSE}
for (i in 1:length(metody)) {
    suppressMessages(library(PairedData))
    suppressMessages(library(profileR))
    pd = paired(as.numeric(acc[1:3, paste0(metody[i], "_train_Accuracy")]), as.numeric(acc[1:3, paste0(metody[i], "_test_Accuracy")]))
    colnames(pd) = c("Train Accuracy", "Test Accuracy")
    plot2 = ks.profileplot(pd, Method.id = acc$method[1:3], standardize = F)
    pd = paired(as.numeric(acc[1:3, paste0(metody[i], "_train_Accuracy")]), as.numeric(acc[1:3, paste0(metody[i], "_valid_Accuracy")]))
    colnames(pd) = c("Train Accuracy", "Valid Accuracy")
    plot3 = ks.profileplot(pd, Method.id = acc$method[1:3], standardize = F)
    pd = paired(as.numeric(acc[1:3, paste0(metody[i], "_test_Accuracy")]), as.numeric(acc[1:3, paste0(metody[i], "_valid_Accuracy")]))
    colnames(pd) = c("Test Accuracy", "Valid Accuracy")
    plot4 = ks.profileplot(pd, Method.id = acc$method[1:3], standardize = F)



    require(gridExtra)
    grid.arrange(arrangeGrob(plot2, plot3, ncol = 2, nrow = 1, top = metody[i]))
    grid.arrange(arrangeGrob(plot4, ncol = 1, nrow = 1, top = metody[i]))
}
```

### Relationship between accuracy on testing and validation sets:

For top 6 methods.

```{r echo = FALSE, message = FALSE, warning = FALSE}
acc2 = acc[1:6, ]  # get top 6 methods
accmelt = melt(acc2, id.vars = "method") %>% filter(variable != "metaindex") %>% filter(variable != "miRy")
accmelt = cbind(accmelt, strsplit2(accmelt$variable, "_"))
acctest = accmelt$value[accmelt$`2` == "test"]
accvalid = accmelt$value[accmelt$`2` == "valid"]
accmeth = accmelt$method[accmelt$`2` == "test"]
plot5 = ggplot(, aes(x = as.numeric(acctest), y = as.numeric(accvalid), shape = accmeth)) + geom_point() + scale_x_continuous(name = "Accuracy on test set",
    limits = c(0.5, 1)) + scale_y_continuous(name = "Accuracy on validation set", limits = c(0.5, 1)) + theme_bw()
grid.arrange(arrangeGrob(plot5, ncol = 1, nrow = 1))
```

## Best feature set

By default we choose the best performing set which achived the best mean accuracy in training, testing and validation. You can customize it in the code below.

```{r echo = FALSE, message = FALSE, warning = FALSE}
acc1 = ks.best_signiture_proposals(benchmark_csv = "benchmark.csv", without_train = F)  # here you can customize how the best signiture is selected.
best_signatures = acc1[1:3, ]  # get top 3 methods
```

Best set:

```{r echo = FALSE, message = FALSE, warning = FALSE}
ks.table(best_signatures[1, 2:4])
```

### DE of selected features:

This should serve as a sanity check.

```{r echo = FALSE, message = FALSE, warning = FALSE}
best_de = ks.best_signiture_de(selected_miRNAs)
ks.table(best_de)
```

### Exploratory analysis best set:

```{r echo = FALSE, message = FALSE, warning = FALSE}
ks.vulcano_plot(selected_miRNAs = de$miR, DE = de, only_label = selected_miRNAs)
```

```{r echo = FALSE, message = FALSE, warning = FALSE}
ks.heatmap(x = dplyr::select(mixed, gsub("\\.", "-", selected_miRNAs)), rlab = data.frame(Class = mixed$Class, Mix = mixed$mix), zscore = F, margins = c(10,
    10), expression_name = "value")
```

```{r echo = FALSE, message = FALSE, warning = FALSE}
ks.heatmap(x = dplyr::select(mixed, gsub("\\.", "-", selected_miRNAs)), rlab = data.frame(Class = mixed$Class, Mix = mixed$mix), zscore = T, margins = c(10,
    10), expression_name = "value")
```